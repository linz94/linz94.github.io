@inproceedings{tomar2021content,
  title = {Content-Preserving Unpaired Translation from Simulated to Realistic Ultrasound Images},
  author = {Tomar, Devavrat and Zhang, Lin and Portenier, Tiziano and Goksel, Orcun},
  abstract = {Interactive simulation of ultrasound imaging greatly facilitates sonography training. Although ray-tracing based methods have shown promising results, obtaining realistic images requires substantial modeling effort and manual parameter tuning. In addition, current techniques still result in a significant appearance gap between simulated images and real clinical scans. Herein we introduce a novel content-preserving image translation framework (ConPres) to bridge this appearance gap, while maintaining the simulated anatomical layout. We achieve this goal by leveraging both simulated images with semantic segmentations and unpaired in-vivo ultrasound scans. Our framework is based on recent contrastive unpaired translation techniques and we propose a regularization approach by learning an auxiliary segmentation-to-real image translation task, which encourages the disentanglement of content and style. In addition, we extend the generator to be class-conditional, which enables the incorporation of additional losses, in particular a cyclic consistency loss, to further improve the translation quality. Qualitative and quantitative comparisons against state-of-the-art unpaired translation methods demonstrate the superiority of our proposed framework.},
  booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
  year = {2021},
  arxiv = {2103.05745},
  preview = {conpres_preview.png},
  bibtex_show = {true},
  selected = {true},
}


@article{zhang2021learning,
  title={Learning Ultrasound Rendering from Cross-sectional Model Slices for Simulated Training},
  author={Zhang, Lin and Portenier, Tiziano and Goksel, Orcun},
  journal={International Journal of Computer Assisted Radiology and Surgery},
  abstract = {Purpose. Given the high level of expertise required for navigation and interpretation of ultrasound images, computational simulations can facilitate the training of such skills in virtual reality. With ray-tracing based simulations, realistic ultrasound images can be generated. However, due to computational constraints for interactivity, image quality typically needs to be compromised.
Methods. We propose herein to bypass any rendering and simulation process at interactive time, by conducting such simulations during a non-time-critical offline stage and then learning image translation from cross-sectional model slices to such simulated frames. We use a generative adversarial framework with a dedicated generator architecture and input feeding scheme, which both substantially improve image quality without increase in network parameters. Integral attenuation maps derived from cross-sectional model slices, texture-friendly strided convolutions, providing stochastic noise and input maps to intermediate layers in order to preserve locality are all shown herein to greatly facilitate such translation task.
Results. Given several quality metrics, the proposed method with only tissue maps as input is shown to provide comparable or superior results to a state-of-the-art that uses additional images of low-quality ultrasound renderings. An extensive ablation study shows the need and benefits from the individual contributions utilized in this work, based on qualitative examples and quantitative ultrasound similarity metrics. To that end, a local histogram statistics based error metric is proposed and demonstrated for visualization of local dissimilarities between ultrasound images.},
  arxiv = {2101.08339},
  year = {2021},
  preview = {ijcars.gif},
  slides = {https://www.youtube.com/watch?v=sBsRKA8eVFM&t=37s},
  bibtex_show = {true},
  selected = {true},
}

@article{zhang2020deep,
  title={Deep Network for Scatterer Distribution Estimation for Ultrasound Image Simulation},
  author={Zhang, Lin and Vishnevskiy, Valery and Goksel, Orcun},
  journal={IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control},
  abstract= {Simulation-based ultrasound training can be an essential educational tool. Realistic ultrasound image appearance with typical speckle texture can be modeled as convolution of a point spread function with point scatterers representing tissue microstructure. Such scatterer distribution, however, is in general not known and its estimation for a given tissue type is fundamentally an ill-posed inverse problem. In this paper, we demonstrate a convolutional neural network approach for probabilistic scatterer estimation from observed ultrasound data. We herein propose to impose a known statistical distribution on scatterers and learn the mapping between ultrasound image and distribution parameter map by training a convolutional neural network on synthetic images. In comparison with several existing approaches, we demonstrate in numerical simulations and with in-vivo images that the synthesized images from scatterer representations estimated with our approach closely match the observations with varying acquisition parameters such as compression and rotation of the imaged domain.},
  year={2020},
  preview = {scatterparam.png},
  arxiv = {2006.10166},
  bibtex_show = {true},
  selected = {true},
}

@inproceedings{zhang2019implicit,
  title={Implicit Modeling with Uncertainty Estimation for Intravoxel Incoherent Motion Imaging},
  author={Zhang, Lin and Vishnevskiy, Valery and Jakab, Andras and Goksel, Orcun},
  abstract={Intravoxel incoherent motion (IVIM) imaging allows contrast-agent free in vivo perfusion quantification with magnetic resonance imaging (MRI). However, its use is limited by typically low accuracy due to low signal-to-noise ratio (SNR) at large gradient encoding magnitudes as well as dephasing artefacts caused by subject motion, which is particularly challenging in fetal MRI. To mitigate this problem, we propose an implicit IVIM signal acquisition model with which we learn full posterior distribution of perfusion parameters using artificial neural networks. This posterior then encapsulates the uncertainty of the inferred parameter estimates, which we validate herein via numerical experiments with rejection-based Bayesian sampling. Compared to state-of-the-art IVIM estimation method of segmented least-squares fitting, our proposed approach improves parameter estimation accuracy by 65% on synthetic anisotropic perfusion data. On paired rescans of in vivo fetal MRI, our method increases repeatability of parameter estimation in placenta by 46%.},
  booktitle={2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)},
  year={2019},
  preview = {agp.png},
  arxiv = {1810.10358},
  bibtex_show = {true},
  selected = {false},
}

@article{zhang2018temporal,
  title={Temporal Interpolation via Motion Field Prediction},
  author={Zhang, Lin and Karani, Neerav and Tanner, Christine and Konukoglu, Ender},
  abstract ={Navigated 2D multi-slice dynamic Magnetic Resonance (MR) imaging enables high contrast 4D MR imaging during free breathing and provides in-vivo observations for treatment planning and guidance. Navigator slices are vital for retrospective stacking of 2D data slices in this method. However, they also prolong the acquisition sessions. Temporal interpolation of navigator slices an be used to reduce the number of navigator acquisitions without degrading specificity in stacking. In this work, we propose a convolutional neural network (CNN) based method for temporal interpolation via motion field prediction. The proposed formulation incorporates the prior knowledge that a motion field underlies changes in the image intensities over time. Previous approaches that interpolate directly in the intensity space are prone to produce blurry images or even remove structures in the images. Our method avoids such problems and faithfully preserves the information in the image. Further, an important advantage of our formulation is that it provides an unsupervised estimation of bi-directional motion fields. We show that these motion fields can be used to halve the number of registrations required during 4D reconstruction, thus substantially reducing the reconstruction time.},
  arxiv={1804.04440},
  journal={Medical Imaging with Deep Learning},
  preview = {mfin.gif},
  year={2018},
  slides = {https://www.youtube.com/watch?v=g3j5i35G3Hw},
  bibtex_show = {true},
  selected = {true},
}
